---
title: "Reproduction of Evidence of Serial Processing in Visual Word Recognition by White, Palmer and Boynton (2018, Psychological Science)"
author: "Jamie Mitchell (jamiel12@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Reproducibility reports should all use this template to standardize reporting across projects. These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

##Introduction

### Description of paper:

This paper describes two experiments that were conducted in which researchers used eye tracking and rapid-serial-visual-presentation of stimuli to determine if the human visual parallel architecture allows for simultaneous processing of two words at once or if word recognition is constrained to serial processing. In Experiment #1 Participants were viewed pairs of words, displayed in rapid succession and were asked to identity the presence or absence of a target word from a pre-identified semantic category.

**Clarify key analysis of interest here**  
The authors computed the area under the receiver-operating-characteristic (ROC) curve, known as Greenâ€™s area or *Ag* as a bias-free measure of accuracy.They then coded all dual-task responses by whether the response to the other side on the same trial was correct or incorrect and then computed *Ag* for both sets of trials.

### Justification for choice of study

This paper uses eye tracking data to help measure word processing ability. The research question and methodology of this paper are very relevant the the research I will be conducting as a PhD student. My research focuses on the neural processing of stimuli associated with reading (primarily withing the visual and auditory systems). Due to the in-person nature of the project and the present state of the world, I will not be able to collect my own data for this project. I will instead be using the raw data from the original paper, which is publicly available on Open Science Framework, to reproduce the results found in this paper.This will be useful in helping me become familiar with with eye tracking data along with developing a workflow for such data. I will also gain experience working through the challenges associated with reproduction of eye tracking data. This paper describes 2 experiements. I will be reproducing the results for Experiment #1.

### Anticipated challenges

I anticipate some challenges processing and analyzing the data as I have no prior experience working with eye-tracker data.I also do not have extensive prior knowledge of R so I anticipate some difficulty analyzing data as I am learning as I go.

### Links

Project repository: https://github.com/psych251/white_2018

Original paper: https://github.com/psych251/white_2018/tree/master/original_paper

Open Science Framework: https://osf.io/ewr45 


## Methods

### Description of the steps required to reproduce the results

To reproduce the findings of the original paper, I will
  1. Download the raw data supplemental materials for Experiment 1 from Open Science Framework
  2. Combine individual participant data into 1 spreadsheet
  3. Clean the data by filtering out trials containing fixation breaksand entire blocks with stimulus difficulty levels set too high.
  4. Compute dual-task accuracy (*Ag*) for each participant, for each side of fixation, for each task
  5. Plot dual-task accuracy of of responses correct on other side against responses incorrect on other side.
  6. Calculate standard error of mean
  7. Calculate t value
  8. Calculate p value
  9. Calculate confidence interval
  

### Differences from original study

The only distinct anticipated difference in this paper from the original study is that the original authors used MatLab to run and analyze the experiment whereas I will be using R. I anticipate that there there may be some minor adjustments made however I am unfamiliar with MatLab and therefore am unable to accurately address if there are any key differences in functionality between the two programs. 


## Results

### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions
library(tidyverse)
library(RCurl)

#### Import data

#temp <- tempfile()
#download.file("https://github.com/psych251/white_2018/blob/master/white2018_data/#Expt1_IndivData%20(1).zip",temp)
#data2 <- read.table(unz(temp, "a1.dat"))
#unlink(temp)

#data_link <- getURL("https://github.com/psych251/white_2018/blob/master/white2018#_data/Expt1_IndivData%20(1).zip")
#data.csv <- read.csv(text = data_link)

# testing methods to load data from github repo - using local directory for now
mypath = "~/Documents/Stanford/Courses/PSYCH_251/white_2018/white2018_data/Expt1_IndivData"
setwd(mypath)

# combine individual subject data into a single master data file
data_ls = list.files(path=mypath, pattern="*.txt") 
txt_files_df <- lapply(data_ls, function(x) {read.delim(file = x, header = T, sep ="")})
data <- do.call("rbind", lapply(txt_files_df, as.data.frame))

#### Data exclusion / filtering
data <- data %>%
  filter(excludeBlock_BadDifficulty!=1,
         fixBreak!=1,
         nResps == 2)

#### Prepare data for analysis - create columns etc.
ROC_data <- subset(data, select=c(blockNum, trial, subjectNum, targSide, whichTask, reportedRating, month, day))

ROC_data <- ROC_data %>%
  mutate(realtrial= ((( ROC_data$blockNum - 1 )*16)+ROC_data$trial))
  


#ROC_sem <-ROC_data %>%
 # filter(whichTask == 1)
  
#ROC_colr <-ROC_data %>%
 # filter(whichTask == 2)
```

### Key analysis

The analyses as specified in the analysis plan.  
```{r}
ROC <- function(df){
  for (participant in levels(factor(df$subjectNum))) {
    
  }
}
  
```

*Side-by-side graph with original graph is ideal here*

###Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Reproduction Attempt

Open the discussion section with a paragraph summarizing the primary result from the key analysis and assess whether you successfully reproduced it, partially reproduced it, or failed to reproduce it.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis of the dataset, (b) assessment of the meaning of the successful or unsuccessful reproducibility attempt - e.g., for a failure to reproduce the original findings, are the differences between original and present analyses ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the reproducibility attempt (if you contacted them).  None of these need to be long.
